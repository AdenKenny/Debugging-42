\chapter{Background and Related Work} \label{C:background}

\section{Compiler Correctness}
A compiler is an extremely large and complex piece of software. As such, it is highly likely that bugs exist in the implementation of practically any language \cite{arnabold} and 42 is no exception to this. Complier testing can be a difficult and time consuming process and Leroy \cite{Leroy:2009} states that compiler-introduced bugs are widely considered to be extremely difficult to find and track down. In this section we will explore some of the current approaches to compiler testing that are considered most important to this project.

\subsection{CompCert}
One of the best ways to truly prove a non-trivial software system is bug free is through formal verification and Miller \cite{Miller:1990} states that the best way to make a systematic statement about the correctness of the code is to use formal methods, but the work required to create a formally verified complier of any real scale is a major barrier to using formal methods in compilers. The only example of a large scale compiler being fully formally verified is CompCert as presented by
Leroy \cite{Leroy:2009}. CompCert is a formally verified compiler for a large subset of the C language \cite{Blazy:2006} \cite{Blazy:2009}. CompCert is a verified compiler that provides some semantic preservation and has
"a mathematical, machine-checked proof that the generated executable code behaves exactly as prescribed by the semantics of the source program" \cite{CompCert}.This, theoretically, rules out the possibility of bugs being introduced by the compiler and means that any bug free code that is compiled will produce a bug free program.

The CompCert compiler is an interesting project, but to carry out a similar project for the 42 language is unrealistic for multiple reasons. First and foremost is that the writing of a formally verified compiler is significantly out of scope of this project, especially considering the CompCert compiler has had over six man-years of effort spent on it. Additionally if the approach used by Leroy was to be applied to a language different to the C subset introduced by Blazy \cite{Blazy:2009} some issues may arise. These issues would relate to verifying some 42 language features such as metaprogramming that could be difficult to verify.


\section{Automated Testing}
\subsection{Fuzzing}
Fuzzing is the technique of supplying random input into a program or application. This technique is quite often considered to be quite simple \cite{Miller:1995} but can be effective as it is fairly easy to implement, especially if existing tools can be used, and can find a large number of bugs\cite{Miller:1990} \cite{Miller:1995} \cite{Miller:2000} \cite{arnabold}. An example of a modern, well used fuzzing tool is \textit{american fuzzy lop} (AFL) \cite{afl}. AFL differs from other older and less advanced fuzzing tools such as \textit{fuzz} \cite{Miller:1990} by the use of compile-time instrumentation and genetic algorithms. This allows AFL to "discover clean, interesting test cases that trigger new internal states in the targeted binary" \cite{afl}. AFL has been used to find bugs in major programs such as \textit{Apple Safari}, \textit{llvm}, \textit{Adobe Reader}, and \textit{VLC} \cite{afl}.

\subsubsection{Potential Fuzzer Usage}
A potential issue with using AFL is that the interface to run it on Java programs \cite{javaAfl} is less used, and less well documented than the standard AFL version.
While fuzzers are a useful tool that have worked well on similar projects such as \textit{llvm}, using a fuzzer on the 42 compiler may present some issues. Perhaps the most problematic issue would be compilation time of a 42 program using the 42 compiler. Compiling a 42 program currently takes a significant period of time if the full standard library is used (around 30 seconds on fairly standard consumer hardware), and when large numbers of programs are compiled, the time taken would quickly add up. Additionally a fuzzer would probably have to be written to properly test 42, and it would have to be quite advanced to provide any meaningful results. If a fuzzer were to be used it would probably not be particularly effective unless significant amounts of time were spent writing it.

\section{General Testing Literature}
\subsection{Compiler Testing}

Unfortunately, literature on compiler testing is scarce \cite{jai}, but this section will briefly discuss some of the papers that do exist in this body of literature.

\subsubsection{Automated Testing}
In a paper presented by Chen et al. the topic of automated compiler testing is introduced \cite{Chen:2017}. They note that traditional automated testing techniques such as fuzzing have performance issues; with many tests and a significant amount of time needed to find bugs. They then introduce the concept of "learning to test" and propose LET, an approach to compiler testing that allows the prioritisation of test programs through a learning process, where a model is trained. This model is then used to prioritise test programs based on their likelihood of finding a bug and their performance costs. The results are quite impressive with around 60\% of test cases being sped up by at least 25\%. While the results are significant, the usage of these findings in regards to the test of the 42 compiler seems unlikely. This is primarily due to the approach being cutting edge, and seemingly difficult and time consuming to replicate or use, event if it were to be available.

In another paper, Chen et al. present a comparison of automated compiler testing techniques \cite{Chen:2016}. They compare three different techniques, Randomized Differential Testing (RDT), Different Optimization Levels (DOL) (a variation of RDT), and Equivalence Modulo Inputs (EMI). The results show that DOL is effective at finding bugs relating to optimisation, and RDT is generally more effective at detecting other types of bugs, but all three techniques can be used in conjunction with one anothe and complement each other. The results presented here are not particularly useful to this project as 42 does not have a heavily optimising compiler or different levels of optimisation, therefore DOL would not be of much use if applied to this project. RDT is used with two different compilers implementing the same specification and 42 only has a single compiler implementation therefore RDT would also not be very useful for this project.

Kossatchev and Posypkin present a survey of "Compiler Testing Methods" \cite{Kossatchev}. A key idea that can be taken from this paper is that "the algorithms for generating syntax-oriented test suites are time-tested ones. There remains a question of whether the use of automated methods for generating semantics-oriented tests ...  is justified from the practical standpoint, since the approaches discussed have been tested on subsets of real languages or on model programming languages". This can be related to 42, where it is clear that automated methods of generating simple programs that test the syntax of the language is a fairly easy task. In contrast it is likely that automated testing of non-syntax, or more "semantic-orientated" parts of the language would be significantly harder. This is due to the fact that it could potentially require the automated tool to "learn" how to generate 42 programs that are valid and complex enough to test anything beyond the parser. This paper raises some doubts over the usefulness of automated compiler testing for this project, especially in regards to the use of fuzzers as discussed previously. It is felt that any fuzzer used on, or created for this project is unlikely to find many bugs.  

 Additionally, the 42 parser was created with ANTLR \cite{antlr}. This means that any bugs that relate to the 42 parser would most likely be ANTLR bugs, which while being significant and definitely worthy of attention, are not the focus of this project.
While it is certainly possible that bugs exist in the 42 parser, and it is known that parsing bugs can be extremely serious, especially from a security perspective \cite{parserBugs}, we feel that testing of other language features such as the type system and syntax are of a higher priority at the present  time.

The development of an automated testing tool is often a significant effort, and generally requires significant amounts of development time and expertise \cite{enc2}. Test automation done well is a long term investment rather than a shortcut to better or cheaper testing \cite{enc2}. Fewster and Graham note that it is generally during the later maintenance of the system, rather than during the development process, that the benefit that automated testing tools provide start to outweigh the development costs \cite{sta}.

\subsection{Traditional Testing}

\subsubsection{Black-box Testing}
Ostrand defines black-box testing as "any method of generating testcases that is independent of the software's internal structure" \cite{ostrand}. Nidhra \cite{nidhra} states that "the main advantage" of black-box testing is that a tester is not required to have knowledge of the programming language a system is implemented in, nor knowledge of the system itself. Nidhra also states that black-box testing aids "in overall functionality validation of the system" \cite{nidhra}. This is important for a compiler as it would prove difficult to have complete knowledge of the overall system of a non-trivial compiler. Therefore, in the case of black-box testing a compiler, a tester would not have to have specific knowledge of the compiler or the source code. They would construct programs and attempt to compile them with the compiler that is being tested. The tester would then compare the expected output of running the compiled code with the actual output, and if the actual output differs from the expected output, a bug has been identified. An issue that must be kept in mind with this style of testing on a compiler is that it is reasonable that the tester will write code that is incorrect, and if the tester does not realise this, it is possible that an identified bug is a false positive.

When black-box testing, the tester should consider the purpose of the program, the possible inputs and outputs, the possible ways the program may fail, and the potential uses of the program \cite{ostrand}. 

A fairly common type of black-box testing is model-based testing, where the tester will create a model of "the process that the software is supposed to carry out" \cite{ostrand}. Finite-state machines are commonly used for modelling state-based and interactive systems. Model based testing can be difficult and complex when attempting to test large and complex systems, as it can be difficult to capture the entirety of the behaviour of the system in the model.

Another major type of black-box testing is use case testing \cite{ostrand}. For use case testing, test cases are constructed from descriptions of how the system should work or process tasks or events. A scenario is created with preconditions that must hold before the scenario takes place, and with postconditions that must hold after the scenario takes place. When the test is run, the outputs are compared against the postconditions for correctness, and if the outputs match postconditions the test has succeeded.

\subsubsection{White-box Testing \label{whitebox}}
Ostrand defines white-box testing as "test methods that rely on the internal structure of the software" \cite{ostrand}. White-box testing methods are designed to cover specific elements of the code by testing that specific element. The ideal case for white-box testing is to "test every part of the code in every way that it could be executed during actual operation of the system"  \cite{enc2}. Unfortunately, this ideal case is only possible in very simple programs with finite input domains \cite{enc2}. In practice, white-box test methods are generally used for checking the effectiveness and adequacy of black-box testing methods \cite{enc2}. At the end of the testing cycle white-box methods are sometimes employed to create tests to fill in the gaps of black-box testing \cite{ostrand}. 

Types of white-box testing methods include statement coverage testing, in which every single statement in the program is executed and checked to make sure the result is correct \cite{enc2}. This results in very thorough coverage of the program but may not take into account different paths that the program may have taken to get to a specific statement. Taking different paths through the program may result in different behaviour.

 Branch coverage testing is similar to statement coverage; however rather than every statement being tested, every possible branch that could be taken in the program is taken and therefore tested. Branch coverage is generally considered to be the bare minimum for a program to be considered thoroughly tested in a white-box methodology \cite{enc2}.

White-box testing often takes the form of unit testing, where the program is tested component by component. A component in this case, is the smallest possible testable part of the program. The smaller and simpler the component, the easier it is to test. 

Marciniak states that a common belief is that test suites with a high branch or path coverage will find a substantial number of bugs, but he states that "this has not been verified with controlled studies of real software" \cite {enc2}.

White-box testing presents the advantage of  being able to expose implementation errors that do not have anything to do with the problem specification. This means that white-box tests may catch bugs that would not be considered when purely thinking in terms of the program specific \cite{enc2}. Additionally, white-box testing has a natural concept of how much of the program is actually tested, through statement and branch coverage statistics.

A white-box testing methodology does present some disadvantages, Marciniak notes that "exercising a particular faulty statement or branch is no guarantee that the test input used is one that actually causes a failure to occur" \cite{enc2}.

\subsubsection{Grey-box testing}

Grey-box testing is the combination of white-box and black-box testing. It attempts to combine the advantages of both white-box and black-box testing while minimising the disadvantages of both \cite{ostrand}.

 Generally, when black-box testing, the tester has no knowledge of the internal nature of the system, and when white-box testing, the tester has complete knowledge of the internal nature and implementation of the system. A grey-box tester should have partial knowledge of the system, and this generally takes the form of a high-level overview of the system and documentation. 

The scope of the tests in grey-box testing are at a black-box level rather than a white-box level. In other words, the tests will test the system as a whole rather than specific parts of the system. The partial knowledge of the system is used to target tests at specific parts of the system. The fact that a tester can target tests at specific parts of the system that they know may contain presents an advantage over purely black-box testing as the tester can use their knowledge of the system to tailor tests to be effective.

Grey-box testing methods generally take the form of black-box testing methods, but with the addition of (generally small amounts of) system knowledge.

\subsubsection{Orthagonal Latin Squares \label{orthsec}}
A paper by Mandl \cite{Mandl} introduces the concept of experiment design on testing. He states when given a test objective, ``we identify a state space spanned by a finite number of variables with a number of allowable values `` \cite{Mandl}. There are then two typical techniques for testing given this state space. The first option is to exhaustively cover the entire state space, and the second option is a random walk through the state space. Both of these techniques have rather major downsides. The exhaustive approach will present issues when the state space is large, as the number of tests required will be large. The random walk approach also presents problems, as the state space will not be fully tested, and ``it is difficult to assess the level of confidence to be derived from the apparently successful testing. " \cite{Mandl}. Mandl then introduces a third approach. This approach uses orthogonal Latin squares \cite{Parker} to test ``\textit{k} variables each admitting \textit{n} values'' \cite{Mandl} (assuming finite values of \textit{k} and \textit{n}). We can create a set of $k - 2$ orthogonal $n \times n$ Latin squares, and from this implement $n^2$ test cases corresponding to the entries of the squares, rather than the $n^k$ test cases that would be required for an exhaustive test. Mandl states that the we gain ``essential exhaustiveness'' at a lower cost. Mandl then cites an example of $n = 4$ and $k = 4$, where the orthogonal Latin squares approach requires $4^2$ test cases compared to the $4^4$ test cases required if we were to perform an exhaustive test. As a final note Mandl states that the  ``orthogonal-Latin-squares method being proposed seems to strike a very efficient compromise between the level of effort required and the amount of information obtained:  At a cost commensurate with that of a traditional set of test cases selected at random, it yields about as much useful information as the prohibitively expensive exhaustive testing.`` \cite{Mandl}. 

This style of testing could prove useful for this project if modifications were made to the method. These modifications would involve choosing test cases and setting an upper bound on the \textit{k} and \textit{n} values. This would be due to the fact that for compiler input it is very easy to imagine scenarios where \textit{k} and \textit{n} are extremely large and would involve very large squares. This proposed modification would mean that the testing would probably not yield as much information as exhaustive testing, but it is possible that it would still be more cost-effective than random testing.




